{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    <h1> INF335 - Tecnologías de Búsqueda Web   </h1>\n",
    "    <h2> Tarea 1 </h2>\n",
    "    <h3> Universidad Técnica Federico Santa Maria </h3>\n",
    "    \n",
    "</center>\n",
    "\n",
    "_10 de Abril de 2017_\n",
    "<p>Sebastián Aranda 201104560-2 <br> Felipe Santander 201104</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Introducción</h1>\n",
    "<p>Se presenta un dataset con alrededor de 560.000 críticas de platos de comida y restaurantes provenientes de Amazon, para el cual se desea realizar procesamiento de texto a las reseñas utilizando la herramienta de python NLTK. Se propone un código compuesto por 3 scripts:</p>\n",
    "<ul>\n",
    "    <li><b>t1.1_tbw.py</b> Construcción del corpus</li>\n",
    "    <li><b>t1.2_tbw.py</b> Filtración de tokens con baja frecuencia, Obtención de collocations, Postagging y Reconocimiento de entidades</li>\n",
    "    <li><b>t1.3_tbw.py</b> Análisis de Sentimientos</li>\n",
    "</ul>\n",
    "<p>Es importante señalar que debido al gran tamaño del dataset provisto se decidió trabajar con un subconjunto de la totalidad de reseñas. Para lo anterior se separó el corpus en 56 subconjuntos con 10.000 reseñas cada uno. Utilizando la libreria Pickle se fueron guardando en el disco las particiones del corpus para su posterior uso.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Procesamiento de Texto</h1>\n",
    "<h2>Construcción del Corpus</h2>\n",
    "<p>Para realizar el procesamiento de texto es necesario construir un Corpus. Para esto se realizaron los pasos descritos a continuación:</p>\n",
    "<ol>\n",
    "\t<li><b>Eiminación de etiquetas HTML</b><p style=\"margin:0\">Utilizando la librearia BeautifulSoup se eliminan los tags HTML presentes en algunas reseñas</p></li>\n",
    "    <li><b>Transformación a minúsculas</b><p style=\"margin:0\">Se transforma cada reseña a minúsculas para coincidir tokens</p></li>\n",
    "    <li><b>Eliminación de signos</b><p style=\"margin:0\">Se establece un listado de simbolos no permitidos y estos son eliminados de cada reseña</p></li>\n",
    "    <li><b>Guardado de estado de corpus</b><p style=\"margin:0\">Cada 10.000 reseñas procesadas se genera un archivo .pkl que guarda los documentos (reseñas), tokens filtrados y frecuencias del corpus generado con la cantidad de reseñas mencionada</p></li>\n",
    "</ol>\n",
    "<p>El proceso anterior se realizó para la generación de un solo Corpus, es decir, se procesaron solo 10.000 reseñas. Esta ejecución tardó <b>49,363 segundos</b>.</p>\n",
    "<h1>Procesamiento Corpus</h1>\n",
    "<p>Debido al elevado tiempo de ejecución de los distintos tipos de procesamiento del corpus se decidió trabajar con el siguiente conjunto de datos:</p>\n",
    "<ul>\n",
    "<li>Corpus de <b>10.000</b> Reseñas (primeras 10.000)</li>\n",
    "<li>Totalidad de tokens del corpus de 10.000 reseñas: <b>384.004</b></li>\n",
    "<li>Cantidad de documentos para reconocimiento de estructuras: <b>100</b></li>\n",
    "<li>Tokens por documento promedio: <b>75</b></li>\n",
    "<li>Tokens por documento máximo: <b>1.464</b></li>\n",
    "<li>Tokens por documento mínimo: <b>10</b></li>\n",
    "</ul>\n",
    "<p>La lectura del corpus demoró <b>18,052 segundos</b>.</p>\n",
    "\n",
    "<h2>Filtración de tokens poco frecuentes</h2>\n",
    "<p>Se realizó un procedimiento similar a la eliminiación de stopwords, pero esta vez considerando las palabras que aparecen en el diccionario de frecuencia filtrado por los tokens que aparecen 3 o más veces.</p>\n",
    "<p>El filtrado anterior tardó <b>454,981 segundos</b>.</p>\n",
    "\n",
    "<h2>Obtención de Collocations</h2>\n",
    "<p>Se procede leyendo el primer Corpus generado con 10.000 reseñas, luego con el listado de tokens se realiza una identificación de bigramas, seleccionando los 30 más frecuentes. Para lo anterior se utilizaron las funciones provistas por nltk para obtención de bigramas.</p>\n",
    "<p>La identificación de bigramas tardó <b>4,943 segundos</b>.</p>\n",
    "\n",
    "<h2>Postagging</h2>\n",
    "<p>Se realizo postagging de 100 documentos. Analizando los resultados obtenidos se aprecia que el algoritmo etiqueta los tokens de manera correcta el x% de los tokens, considerando un conjunto de y tokens.</p>\n",
    "<p>El procesamiento anterior demoró en promedio <b>6,507 segundos</b> por documento, si se hubiese intentado procesar el corpus seleccionado (10.000 documentos) el tiempo de ejecución hubiese sido de <b>18 horas</b> aprox.</p>\n",
    "\n",
    "<h2>Reconocimiento de Entidades</h2>\n",
    "<p>Al igual que para Postagging, se realizó el reconocimiento de entidades para 100 reseñas. Sin embargo se logra notar que es un poco más dificil reconocer entidades, demorando <b>7,872 segundos</b> por documento en promedio, por lo tanto se hubiese demorado alrededor de 20 horas en procesar todos los docuemntos del corpus</p>\n",
    "<p>En cuanto a la eficacia del algoritmo se observa un x% de exactitud</p>\n",
    "<p>En un principio se intentó realizar el reconocimiento con los documentos en minúsculas y el algoritmo no produjo ningún efecto.</p>\n",
    "\n",
    "<h2>Análisis de Sentimientos</h2>\n",
    "<p></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Conclusiones</h1>\n",
    "<p>Se concluye...</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1>Anexo</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><b>t1.1_tbw.py</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import pickle\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "puncts = \".,:;?!()[]{}~+-\\\"\\'#$%&\"\n",
    "#puncts = \".,?!()[]~-\\\"\\'&1234567890\"\n",
    "\n",
    "file_path = './amazon-fine-foods/Reviews.csv'\n",
    "\n",
    "timer = time.time()*1000\n",
    "mcorpus = dict()\n",
    "mcorpus['documents']= list()  #Corpus para guardar resenias\n",
    "mcorpus['tokens'] = list() #Corpus para guardar tokens\n",
    "mcorpus['token_freq'] = defaultdict(int) #Diccionario para contar repeticiones de tokens\n",
    "i=-1\n",
    "j=0\n",
    "n = 10000 #Documentos procesados\n",
    "with open(file_path, 'rb') as csvfile:\n",
    "\tspamreader = csv.reader(csvfile, delimiter=',')\n",
    "\tfor row in spamreader: \n",
    "\t\ti += 1\n",
    "\t\tif i==0:\n",
    "\t\t\tcontinue\n",
    "\t\telif i>n:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\t#Eliminando etiquetas\n",
    "\t\ttext = BeautifulSoup(row[9], 'html.parser').getText()\n",
    "\n",
    "\t\t#Guardando documento\n",
    "\t\tmcorpus['documents'].append(text.split())\n",
    "\t\t\n",
    "\t\t#Pasando a minusculas\n",
    "\t\ttext = text.lower()\n",
    "\n",
    "\t\t#Eliminando signos y puntuaciones\n",
    "\t\tfor sym in puncts:\n",
    "\t\t\ttext = text.replace(sym, ' ')\n",
    "\n",
    "\t\t#Eliminando Stopwords\n",
    "\t\tfiltered = [token for token in text.split() if token not in stop]\n",
    "\t\t\t\n",
    "\t\tfor token in filtered:\n",
    "\t\t\tmcorpus['tokens'].append(token)\n",
    "\t\t\tmcorpus['token_freq'][token] += 1\n",
    "\n",
    "\t\tsys.stdout.write(\"\\rDocumentos Procesados: \"+str(i)+\"/\"+str(n))\n",
    "\t\tsys.stdout.flush()\n",
    "\n",
    "\t\t# Guardando avance en pickle cada 10000 registros\n",
    "\t\tif i%10000 == 0:\n",
    "\t\t\tj += 1\n",
    "\t\t\tpickle_file_path = './corpus/corpus_'+str(j)+'.pkl'\n",
    "\t\t\toutput = open(pickle_file_path, 'wb')\n",
    "\t\t\tpickle.dump(mcorpus, output)\n",
    "\t\t\toutput.close()\n",
    "\t\t\t\n",
    "\t\t\tmcorpus['documents']= list()  #Corpus para guardar resenias\n",
    "\t\t\tmcorpus['tokens'] = list() #Corpus para guardar tokens\n",
    "\t\t\tmcorpus['token_freq'] = defaultdict(int) #Diccionario para contar repeticiones de tokens\n",
    "\t\t\tgc.collect()\n",
    "\t\n",
    "\ttimer = time.time()*1000-timer\n",
    "\tprint('\\nTiempo de procesamiento: ' + str(timer) + ' ms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><b>t1.2_tbw.py</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "from nltk.collocations import *\n",
    "from collections import defaultdict\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "#Funcion que elimina token con repeticiones menores a x\n",
    "def dict_tokens_modified(data, x):\n",
    "    new_dict_tokens = {k: v for k, v in data.iteritems() if v >= x}\n",
    "    return new_dict_tokens\n",
    "\n",
    "log = {'corpus_read_time': 0, 'filtering_infrequent_tokens_time': 0, 'top30bigrams_time': 0, 'postagger_time': 0, 'nertagger_time': 0}\n",
    "\n",
    "total_timer = time.time()*1000\n",
    "\n",
    "#Leyendo corpus desde archivo pickle (10.000 registros) creado previamente\n",
    "print(\"Leyendo Corpus...\")\n",
    "corpus_timer = time.time()*1000\n",
    "bigcorpus = dict()\n",
    "bigcorpus['documents']= list()  #Corpus para guardar resenias\n",
    "bigcorpus['tokens'] = list() #Corpus para guardar tokens\n",
    "bigcorpus['token_freq'] = defaultdict(int) #Diccionario para contar repeticiones de tokens\n",
    "tokens_per_document = list()\n",
    "for i in range(1):\n",
    "\ti += 1\n",
    "\tpkl_file = open('./corpus/corpus_'+str(i)+'.pkl', 'rb')\n",
    "\tmcorpus = pickle.load(pkl_file) \n",
    "\tpkl_file.close()\n",
    "\n",
    "\tbigcorpus['documents'] += mcorpus['documents'][:100]\n",
    "\tfor doc in mcorpus['documents']:\n",
    "\t\ttokens_per_document.append(len(doc))\n",
    "\tbigcorpus['tokens'] += mcorpus['tokens']\n",
    "\tbigcorpus['token_freq'] = {k: bigcorpus['token_freq'].get(k,0)+mcorpus['token_freq'].get(k,0) for k in set(bigcorpus['token_freq']) | set(mcorpus['token_freq'])}\n",
    "\n",
    "corpus_timer = time.time()*1000 - corpus_timer\n",
    "log['corpus_read_time'] = corpus_timer\n",
    "\n",
    "print(\"Largo Conjunto de Tokens del Corpus: \"+str(len(bigcorpus['tokens'])))\n",
    "print(\"Tokens por Documento Promedio: \"+str(sum(tokens_per_document)/len(tokens_per_document)))\n",
    "print(\"Tokens por Documento Maximo: \"+str(max(tokens_per_document)))\n",
    "print(\"Tokens por Documento Minimo: \"+str(min(tokens_per_document)))\n",
    "\n",
    "#Eliminando palabras con frecuencia menor a 3\n",
    "print(\"Filtrando palabras con frecuencia menor a 3...\")\n",
    "filter_infreq_timer = time.time()*1000\n",
    "filtered_dict = dict_tokens_modified(bigcorpus['token_freq'], 3)\n",
    "validated_tokens = list(filtered_dict)\n",
    "bigcorpus['tokens'] = [token for token in bigcorpus['tokens'] if token in validated_tokens]\n",
    "bigcorpus['token_freq'] = filtered_dict\n",
    "\n",
    "filter_infreq_timer = time.time()*1000-filter_infreq_timer\n",
    "log['filtering_infrequent_tokens_time'] = filter_infreq_timer\n",
    "\n",
    "#Determinando Top30 Collocations\n",
    "print(\"Determinando Top30 Collocations...\")\n",
    "bigram_timer = time.time()*1000\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(bigcorpus['tokens'])\n",
    "finder.apply_freq_filter(3)\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 30)\n",
    "\n",
    "bigram_timer = time.time()*1000 - bigram_timer\n",
    "log['top30bigrams_time'] = bigram_timer\n",
    "\n",
    "print('\\nTOP 30 Collocations Amazon Fine Foods Reviews\\n')\n",
    "i = 0\n",
    "for bigram in bigrams:\n",
    "\ti += 1\n",
    "\tprint str(i)+'. '+str(bigram)\n",
    "\n",
    "print('\\n')\n",
    "#Ejecutando Postagger y Nertagger con n documentos\n",
    "postagger = StanfordPOSTagger('./stanford-postagger/models/english-bidirectional-distsim.tagger', './stanford-postagger/stanford-postagger.jar', encoding='utf-8')\n",
    "nertagger = StanfordNERTagger('./stanford-nertagger/classifiers/english.all.3class.distsim.crf.ser.gz', './stanford-nertagger/stanford-ner.jar', encoding='utf-8')\n",
    "postagger_times = list()\n",
    "nertagger_times = list()\n",
    "i = 0\n",
    "n = 100 #Cantidad de documentos a taggear\n",
    "print(\"Ejecutando Postagger y Nertagger...\")\n",
    "for doc in bigcorpus['documents'][i:n]:\n",
    "\ti += 1\n",
    "\tpickle_file_path_postag = './postags/postagged_'+str(i)+'.pkl'\n",
    "\tpostagger_time = time.time()*1000\n",
    "\tpostagged = postagger.tag(doc)\n",
    "\tpostagger_time = time.time()*1000 - postagger_time\n",
    "\tpostagger_times.append(postagger_time)\n",
    "\toutput = open(pickle_file_path_postag, 'wb')\n",
    "\tpickle.dump(postagged, output)\n",
    "\toutput.close()\n",
    "\n",
    "\tpickle_file_path_nertag = './nertags/nertagged_'+str(i)+'.pkl'\n",
    "\tnertagger_time = time.time()*1000\n",
    "\tnertagged = nertagger.tag(doc)\n",
    "\tnertagger_time = time.time()*1000 - nertagger_time\n",
    "\tnertagger_times.append(nertagger_time)\n",
    "\toutput = open(pickle_file_path_nertag, 'wb')\n",
    "\tpickle.dump(nertagged, output)\n",
    "\n",
    "\tpostagged = None\n",
    "\tnertagged = None\n",
    "\tgc.collect()\n",
    "\n",
    "\tsys.stdout.write(\"\\rDocumentos Procesados: \"+str(i)+\"/\"+str(n))\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "log['postagger_time'] = sum(postagger_times)/len(postagger_times)\n",
    "log['nertagger_time'] = sum(nertagger_times)/len(nertagger_times)\n",
    "\n",
    "pickle_file_path = './log.pkl'\n",
    "output = open(pickle_file_path, 'wb')\n",
    "pickle.dump(log, output)\n",
    "output.close()\n",
    "print('\\n')\n",
    "\n",
    "print(log)\n",
    "total_timer = time.time()*1000 - total_timer\n",
    "print('Largo corpus documentos: '+str(len(bigcorpus['documents']))+ ' Largo corpus tokens:'+str(len(bigcorpus['tokens'])))\n",
    "print(\"Tiempo de procesamiento Total: \"+str(total_timer) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p><b>t1.3_tbw.py</b></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gc\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "from collections import defaultdict\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "total_timer = time.time()*1000\n",
    "\n",
    "#Leyendo corpus desde archivo pickle (10.000 registros) creado previamente\n",
    "print(\"Leyendo Corpus...\")\n",
    "corpus_timer = time.time()*1000\n",
    "bigcorpus = dict()\n",
    "bigcorpus['documents'] = list()\n",
    "for i in range(1):\n",
    "\ti += 1\n",
    "\tpkl_file = open('./corpus/corpus_'+str(i)+'.pkl', 'rb')\n",
    "\tmcorpus = pickle.load(pkl_file) \n",
    "\tpkl_file.close()\n",
    "\tbigcorpus['documents'] += mcorpus['documents'][:100]\n",
    "\n",
    "corpus_timer = time.time()*1000 - corpus_timer\n",
    "print('\\nTiempo de procesamiento: ' + str(corpus_timer) + ' ms')\n",
    "\n",
    "#Entrena el sentiment_analyzer para ser usado en el corpus de resenias\n",
    "print(\"Entrenando sentiment_analyzer...\")\n",
    "n_instances = 1000\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "train_subj_docs = subj_docs[:750]\n",
    "test_subj_docs = subj_docs[750:1000]\n",
    "train_obj_docs = obj_docs[:750]\n",
    "test_obj_docs = obj_docs[750:1000]\n",
    "sentiment_analyzer = SentimentAnalyzer()\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "test_docs = test_subj_docs+test_obj_docs\n",
    "training_set = sentiment_analyzer.apply_features(training_docs)\n",
    "test_set = sentiment_analyzer.apply_features(test_docs)\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyzer.train(trainer, training_set)\n",
    "for key,value in sorted(sentiment_analyzer.evaluate(test_set).items()):\n",
    "\tprint('{0}: {1}'.format(key, value))\n",
    "\n",
    "#Se evalua la polaridad de las resenias cargadas en bigcorpus['documents']\n",
    "print(\"Evaluando...\")\n",
    "pickle_file_path = './polarity/polarity.pkl'\n",
    "output = open(pickle_file_path, 'wb')\n",
    "stop = stopwords.words('english')\n",
    "polarity = list()\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "for sent in bigcorpus['documents']:\n",
    "        lower = [token.lower() for token in sent]\n",
    "        sentence_filtered = [token for token in lower if token not in stop]\n",
    "        ss = sid.polarity_scores(\" \".join(sentence_filtered))\n",
    "        polarity.append(ss)\n",
    "        #Descomentar para tener resultados por pantalla\n",
    "        for k in sorted(ss):\n",
    "                print('{0} : {1}'.format(k, ss[k]))\n",
    "pickle.dump(polarity, output)\n",
    "output.close()\n",
    "gc.collect()\n",
    "print(\"Terminado\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
