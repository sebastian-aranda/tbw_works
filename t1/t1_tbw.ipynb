{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<center>\n",
    "    <h1> INF335 - Tecnologías de Búsqueda Web   </h1>\n",
    "    <h2> Tarea 1 </h2>\n",
    "    <h3> Universidad Técnica Federico Santa Maria </h3>\n",
    "    \n",
    "</center>\n",
    "\n",
    "_Marzo 2017_\n",
    "<p>Profesor: Marcelo Mendoza</p>\n",
    " <p>Ayudante: Daniel Rivera</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> Objetivos </h2>\n",
    "<ul>\n",
    "<li  style =\"margin: 12px 0px; font-size:16px\"> Implementar y analizar la herramienta de python NLTK (Natural Language Tookit) para trabajar con procesamiento de texto y lenguaje natural. </li>\n",
    "<li style =\"margin: 12px 0px;font-size:16px \" > Estudiar e implementar las estructuras de datos adecuadas para representar un corpus, documentos y palabras con su categorización correspondiente.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Dataset : Amazon Fine Food Review</h2>\n",
    "\n",
    "<p style=\"font-size:16px\"> Para esta tarea se va a trabajar con el dataset de <i>“Amazon Fine Food Review”</i> el cual contiene más de 500.000 críticas de platos de comida y restaurants provenientes de Amazon. El archivo consiste en un .csv (“Comma Separate Values”) el cual contiene la siguiente estructura:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<ol>\n",
    "<li style =\"margin: 5px 0px;\"> <strong>Id</strong> - Id único de cada reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong> ProductId</strong> - Id único que identifica el producto a analizar</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>UserId</strong> - Id único que identifica al usuario</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>ProfileName</strong> - Nombre del usuario que realizó la reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>HelpfulnessNumerator</strong> -  número de usuarios que indicaron que encontraron esta crítica util</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>HelpfulnessDenominator</strong> número de usuarios que indicaron que encontraron esta crítica util -</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Score</strong> - Rating, con valores entre 1 y 5 estrellas</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Time</strong> - timestamp for the review</li>\n",
    "<li style =\"margin: 8px 0px;\" ><strong>Summary</strong> - breve resumen de la reseña</li>\n",
    "<li style =\"margin: 8px 0px;\"><strong>Text</strong> - string que contiene la reseña </li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<p style=\"font-size:16px\" > <strong> Link Descarga Dataset: https://drive.google.com/open?id=0B1GNvIDVzwwLR2dwQVliRnBWMnM </strong>  </p> \n",
    "\n",
    "<p style=\"font-size:16px\" > <b> Objetivo: </b> Extraiga del documento el item “Text” y generé un corpus , almacenando en un string todas las reseñas del dataset . Usará esta variable para realizar las siguientes etapas de preprocesamiento de texto.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Preprocesamiento:</h2>\n",
    "\n",
    "<ol >\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > Si observa el corpus, se dará cuenta de que hay etiquetas <i>html</i> embebidas en algunas reseñas. Para eliminar estas etiquetas , use la libreria <i>Beautiful Soup</i> (link: https://www.crummy.com/software/BeautifulSoup/bs4/doc/).</p> </li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\"> Convierta el corpus , de modo que solo existan minúsculas (<code>lowercase</code>). </p></li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando la lista de stopwords ortorgada por nltk, elimine aquellas palabras que sean clasificadas como stopwords, es decir, aquellas palabras que poseen poco contexto léxico y no otorgan información relevante. </p></li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Elimine las palabras que aparezcan en el corpus con una frecuencia inferior a un umbral definido (ejemplo : inferior a 3) ( para ello, es recomendable determinar previo la frecuencia de cada término usando un diccionario). </p></li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando nltk, determine los Top-30 collocations mas relevantes del corpus, usando Bigramas .Implemente la función <code>BigramAssocMeasures()</code> y <code>BigramCollocationFinder.from_words()</code>. Recuerde que para este punto el corpus debe estar tokenizado. (mirar documentación). </p></li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando la libreria incorporada en nltk, implemente Stanford POS tagger para categorizar y obtener los tags de cada token del corpus usando Part-Of-Speech Tagger (POSTagger). </p></li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > Usando la libreria incorporada en nltk, implemente Named Entity Recognition (NER) con Stanford NER Tagger. Analice y describa sus resultados. </p></li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\">\n",
    "<p style=\"font-size:16px\" > <strong> Sentiment Analysis </strong>: Implemente usando la libreria <i>Vader</i> incorporada en nltk para analizar la polaridad del corpus ,determinar cada documento (para ello es necesario re-estructurar el corpus como un array de documentos, o sentencias):</p>\n",
    "<ol>\n",
    "<li> Tokenizar el corpus a nivel de sentencia (recuerde incorporar el preprocesamiento previo).</li>\n",
    "<li> Para cada sentencias (reseña) , implemente Vader para determinar la polaridad.</li>\n",
    "</ol>\n",
    "</li>\n",
    "\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Notas</h2>\n",
    "<ul>\n",
    "<li style =\"margin: 12px 0px; font-size: 16px\" >\n",
    "Para varias etapas del preprocesamiento, usará diferentes librerias disponibles en Python. Se recomienda usar el instalador de paquetes <i>pip</i> ( link: https://pypi.python.org/pypi/pip ) .\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px; font-size: 16px\" >\n",
    "Algunos de estos pasos del preprocesamiento pueden demorar en compilar (en algunos casos sobre 45 min, dependiendo de la máquina), por lo que es recomendable ir guardando el estado del corpus su posterior uso. Para estos casos se recomienda usar la libreria <code> pickle </code> en python (link: https://docs.python.org/2/library/pickle.html ) \n",
    "</li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Documentación y Ejemplos</h2>\n",
    "<ul>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Beautiful Soup :\n",
    "https://www.crummy.com/software/BeautifulSoup/\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Bigrams and Collocations:\n",
    "http://www.nltk.org/howto/collocations.html\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Stanford PoS Tagger :\n",
    "http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford\n",
    "</li>\n",
    "<li>\n",
    "Stanford 'Tagger' Link Download : https://nlp.stanford.edu/software/tagger.shtml#Download\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Stanford Ner Tagger:\n",
    "https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/\n",
    "</li>\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "Sentiment Analysis with Vader: \n",
    "http://www.nltk.org/howto/sentiment.html\n",
    "</li>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2>Instrucciones</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<ol >\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > El informe debe entregarse en un archivo jupyter notebook (diferente a este) con el código  implementado y los análisis correspondientes. El informe debe subirse en la plataforma oficial de moodle en formato comprimido (.zip) con el nombre <i>tarea1_rol.zip</i></p> </li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > Todas las consultas serán atendidas por el canal de consultas de moodle. </i></p> </li>\n",
    "\n",
    "<li style =\"margin: 12px 0px;\" >\n",
    "<p style=\"font-size:16px\" > La fecha de entrega es el dia <strong>10 de Abril</strong> . Pasada esa fecha se descontaran 20 puntos por dia. </p> </li>\n",
    "\n",
    "\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo Corpus...\n",
      "Largo Conjunto de Tokens del Corpus: 384004\n",
      "Tokens por Documento Promedio: 75\n",
      "Tokens por Documento Maximo: 1464\n",
      "Tokens por Documento Minimo: 10\n",
      "Filtrando palabras con frecuencia menor a 3...\n",
      "Determinando Top30 Collocations...\n",
      "\n",
      "TOP 30 Collocations Amazon Fine Foods Reviews\n",
      "\n",
      "1. (u'cesar', u'softies')\n",
      "2. (u'dong', u'quai')\n",
      "3. (u'hash', u'browns')\n",
      "4. (u'lhasa', u'apso')\n",
      "5. (u'nong', u'shim')\n",
      "6. (u'pirate', u'booty')\n",
      "7. (u'ali', u'julia')\n",
      "8. (u'aloe', u'vera')\n",
      "9. (u'ass', u'kickin')\n",
      "10. (u'deficiency', u'anemia')\n",
      "11. (u'douwe', u'egberts')\n",
      "12. (u'juan', u'valdez')\n",
      "13. (u'mead', u'johnson')\n",
      "14. (u'pellegrino', u'aranciata')\n",
      "15. (u'willy', u'wonka')\n",
      "16. (u'humane', u'society')\n",
      "17. (u'kenya', u'aa')\n",
      "18. (u'mt', u'kofinas')\n",
      "19. (u'reporter', u'salem')\n",
      "20. (u'salem', u'statesman')\n",
      "21. (u'lip', u'smacking')\n",
      "22. (u'marinated', u'morsals')\n",
      "23. (u'veronastarbucks', u'pike')\n",
      "24. (u'amy', u'brian')\n",
      "25. (u'castor', u'pollux')\n",
      "26. (u'jay', u'robb')\n",
      "27. (u'medaglia', u'oro')\n",
      "28. (u'propylene', u'glycol')\n",
      "29. (u'statesman', u'journal')\n",
      "30. (u'williams', u'sonoma')\n",
      "\n",
      "\n",
      "Ejecutando Postagger y Nertagger...\n",
      "Documentos Procesados: 100/100\n",
      "\n",
      "{'corpus_read_time': 17784.853271484375, 'top30bigrams_time': 5044.114990234375, 'filtering_infrequent_tokens_time': 474961.61328125, 'postagger_time': 7350.376694335938, 'nertagger_time': 8720.595026855468}\n",
      "Largo corpus documentos: 100 Largo corpus tokens:369644\n",
      "Tiempo de procesamiento Total: 2124280.88721 ms\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "import gc\n",
    "import time\n",
    "import sys\n",
    "from nltk.collocations import *\n",
    "from collections import defaultdict\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "#Funcion que elimina token con repeticiones menores a x\n",
    "def dict_tokens_modified(data, x):\n",
    "    new_dict_tokens = {k: v for k, v in data.iteritems() if v >= x}\n",
    "    return new_dict_tokens\n",
    "\n",
    "log = {'corpus_read_time': 0, 'filtering_infrequent_tokens_time': 0, 'top30bigrams_time': 0, 'postagger_time': 0, 'nertagger_time': 0}\n",
    "\n",
    "total_timer = time.time()*1000\n",
    "\n",
    "#Leyendo corpus desde archivo pickle (10.000 registros) creado previamente\n",
    "print(\"Leyendo Corpus...\")\n",
    "corpus_timer = time.time()*1000\n",
    "bigcorpus = dict()\n",
    "bigcorpus['documents']= list()  #Corpus para guardar resenias\n",
    "bigcorpus['tokens'] = list() #Corpus para guardar tokens\n",
    "bigcorpus['token_freq'] = defaultdict(int) #Diccionario para contar repeticiones de tokens\n",
    "tokens_per_document = list()\n",
    "for i in range(1):\n",
    "\ti += 1\n",
    "\tpkl_file = open('./corpus/corpus_'+str(i)+'.pkl', 'rb')\n",
    "\tmcorpus = pickle.load(pkl_file) \n",
    "\tpkl_file.close()\n",
    "\n",
    "\tbigcorpus['documents'] += mcorpus['documents'][:100]\n",
    "\tfor doc in mcorpus['documents']:\n",
    "\t\ttokens_per_document.append(len(doc))\n",
    "\tbigcorpus['tokens'] += mcorpus['tokens']\n",
    "\tbigcorpus['token_freq'] = {k: bigcorpus['token_freq'].get(k,0)+mcorpus['token_freq'].get(k,0) for k in set(bigcorpus['token_freq']) | set(mcorpus['token_freq'])}\n",
    "\n",
    "corpus_timer = time.time()*1000 - corpus_timer\n",
    "log['corpus_read_time'] = corpus_timer\n",
    "\n",
    "print(\"Largo Conjunto de Tokens del Corpus: \"+str(len(bigcorpus['tokens'])))\n",
    "print(\"Tokens por Documento Promedio: \"+str(sum(tokens_per_document)/len(tokens_per_document)))\n",
    "print(\"Tokens por Documento Maximo: \"+str(max(tokens_per_document)))\n",
    "print(\"Tokens por Documento Minimo: \"+str(min(tokens_per_document)))\n",
    "#Eliminando palabras con frecuencia menor a 3\n",
    "print(\"Filtrando palabras con frecuencia menor a 3...\")\n",
    "filter_infreq_timer = time.time()*1000\n",
    "filtered_dict = dict_tokens_modified(bigcorpus['token_freq'], 3)\n",
    "validated_tokens = list(filtered_dict)\n",
    "bigcorpus['tokens'] = [token for token in bigcorpus['tokens'] if token in validated_tokens]\n",
    "bigcorpus['token_freq'] = filtered_dict\n",
    "\n",
    "filter_infreq_timer = time.time()*1000-filter_infreq_timer\n",
    "log['filtering_infrequent_tokens_time'] = filter_infreq_timer\n",
    "\n",
    "#Determinando Top30 Collocations\n",
    "print(\"Determinando Top30 Collocations...\")\n",
    "bigram_timer = time.time()*1000\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(bigcorpus['tokens'])\n",
    "finder.apply_freq_filter(3)\n",
    "bigrams = finder.nbest(bigram_measures.pmi, 30)\n",
    "\n",
    "bigram_timer = time.time()*1000 - bigram_timer\n",
    "log['top30bigrams_time'] = bigram_timer\n",
    "\n",
    "print('\\nTOP 30 Collocations Amazon Fine Foods Reviews\\n')\n",
    "i = 0\n",
    "for bigram in bigrams:\n",
    "\ti += 1\n",
    "\tprint str(i)+'. '+str(bigram)\n",
    "\n",
    "print('\\n')\n",
    "#Ejecutando Postagger y Nertagger con n documentos\n",
    "postagger = StanfordPOSTagger('./stanford-postagger/models/english-bidirectional-distsim.tagger', './stanford-postagger/stanford-postagger.jar', encoding='utf-8')\n",
    "nertagger = StanfordNERTagger('./stanford-nertagger/classifiers/english.all.3class.distsim.crf.ser.gz', './stanford-nertagger/stanford-ner.jar', encoding='utf-8')\n",
    "postagger_times = list()\n",
    "nertagger_times = list()\n",
    "i = 0\n",
    "n = 100 #Cantidad de documentos a taggear\n",
    "print(\"Ejecutando Postagger y Nertagger...\")\n",
    "for doc in bigcorpus['documents'][i:n]:\n",
    "\ti += 1\n",
    "\tpickle_file_path_postag = './postags/postagged_'+str(i)+'.pkl'\n",
    "\tpostagger_time = time.time()*1000\n",
    "\tpostagged = postagger.tag(doc)\n",
    "\tpostagger_time = time.time()*1000 - postagger_time\n",
    "\tpostagger_times.append(postagger_time)\n",
    "\toutput = open(pickle_file_path_postag, 'wb')\n",
    "\tpickle.dump(postagged, output)\n",
    "\toutput.close()\n",
    "\n",
    "\tpickle_file_path_nertag = './nertags/nertagged_'+str(i)+'.pkl'\n",
    "\tnertagger_time = time.time()*1000\n",
    "\tnertagged = nertagger.tag(doc)\n",
    "\tnertagger_time = time.time()*1000 - nertagger_time\n",
    "\tnertagger_times.append(nertagger_time)\n",
    "\toutput = open(pickle_file_path_nertag, 'wb')\n",
    "\tpickle.dump(nertagged, output)\n",
    "\n",
    "\tpostagged = None\n",
    "\tnertagged = None\n",
    "\tgc.collect()\n",
    "\n",
    "\tsys.stdout.write(\"\\rDocumentos Procesados: \"+str(i)+\"/\"+str(n))\n",
    "\tsys.stdout.flush()\n",
    "\n",
    "log['postagger_time'] = sum(postagger_times)/len(postagger_times)\n",
    "log['nertagger_time'] = sum(nertagger_times)/len(nertagger_times)\n",
    "\n",
    "pickle_file_path = './log.pkl'\n",
    "output = open(pickle_file_path, 'wb')\n",
    "pickle.dump(log, output)\n",
    "output.close()\n",
    "print('\\n')\n",
    "\n",
    "print(log)\n",
    "total_timer = time.time()*1000 - total_timer\n",
    "print('Largo corpus documentos: '+str(len(bigcorpus['documents']))+ ' Largo corpus tokens:'+str(len(bigcorpus['tokens'])))\n",
    "print(\"Tiempo de procesamiento Total: \"+str(total_timer) + \" ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "2.0.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
